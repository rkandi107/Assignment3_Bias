# Assignment3_Bias
explore the concept of bias through querying an existing natural language processing model â€” specifically, the Perspective API released by Google Jigsaw

The objective of this exercise was to explore and identify any potential biases in the Perspective API's toxicity scoring system. My hypothesis was that the model might exhibit biases against certain names/ideas/items that are stereotypically associated with specific ethnicities, genders, and culture. We crafted a series of neutral sentences, each containing a different name that might stereotypically be associated with certain groups. We maintained a consistent structure across sentences to isolate the variable of the name. These sentences were then passed through the Perspective API, and the toxicity scores were recorded. 

The results of my tests were intersting as I found that certain names did indeed return higher toxicity scores, despite the neutrality of the sentence. For instance, names that are commonly associated with ethnic minorities received higher scores on average compared to names that are considered ethnically neutral or associated with white individuals. One surprising outcome was the variation in scores for gendered names. While I expected some difference the amount of it was unexpected, with female-associated names receiving lower toxicity scores than male names, in general.

My theory is that the training data used to develop the Perspective API probably had historical biases andince machine learning models learn from existing data, any biases in said data can be learned by the model. This represents a well-known challenge in AI. This exercise made me think about the nature of bias in machine learning models, like how do developers even make sure their models are fair and unbiased. What are the ethical considerations when using these models in real-world applications?

In conclusion, this assignment looking into the Perspective API provides a look into how biases can develop in AI systems. My findings also suggested that biases  exist and can influence the model's behavior. Addressing these biases is crucial for developing ethical AI systems that can be trusted to make impartial decisions.
